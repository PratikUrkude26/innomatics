{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automated-newspaper",
   "metadata": {},
   "source": [
    "# Classification on CIFAR 10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-vegetable",
   "metadata": {},
   "source": [
    "Instructions - \n",
    "1. In order to kick start, code for setting up the dataframes has already been given (Step 1 to 5)\n",
    "2. Go through the entire code written below and put the relavent comment for each line of code. Understanding below mentioned code is very important.\n",
    "3. After commenting the code in step 1 to 5, complete step 6 and 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-present",
   "metadata": {},
   "source": [
    "### Step - 1 (Setup Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iraqi-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from builtins import range\n",
    "from six.moves import cPickle as pickle\n",
    "from imageio import imread\n",
    "import platform\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-catalyst",
   "metadata": {},
   "source": [
    "### Step - 2 (Download the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recovered-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use this code to download the dataset for the first time\n",
    "# # You will be required to install wget using pip\n",
    "# # For Linux users checkout the alternative for 'del'\n",
    "\n",
    "# !wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -O cifar-10-python.tar.gz\n",
    "# !tar -xzvf cifar-10-python.tar.gz\n",
    "# !del cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-sword",
   "metadata": {},
   "source": [
    "### Step - 3 (Load the Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rotary-colorado",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cifar-10-batches-py\\\\data_batch_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-778a8bc3df5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_CIFAR10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcifar10_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# As a sanity check, we print out the size of the training and test data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-778a8bc3df5d>\u001b[0m in \u001b[0;36mload_CIFAR10\u001b[1;34m(ROOT)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mROOT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data_batch_%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_CIFAR_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mxs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-778a8bc3df5d>\u001b[0m in \u001b[0;36mload_CIFAR_batch\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_CIFAR_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;34m\"\"\" load single batch of cifar \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mdatadict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatadict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cifar-10-batches-py\\\\data_batch_1'"
     ]
    }
   ],
   "source": [
    "# This cell loads the training and testing dataset. Please note the variables at\n",
    "# the end of the cell as you would require them to access the train/test data \n",
    "# and labels throughout the assignment\n",
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == '2':\n",
    "        return  pickle.load(f)\n",
    "    elif version[0] == '3':\n",
    "        return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))\n",
    "\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "\n",
    "cifar10_dir = 'cifar-10-batches-py'\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times\n",
    "try:\n",
    "    del X_train, y_train\n",
    "    del X_test, y_test\n",
    "    print('Clear previously loaded data.')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-dairy",
   "metadata": {},
   "source": [
    "### Step - 4 (Visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-plaintiff",
   "metadata": {},
   "source": [
    "Next we visualize the CIFAR-10 dataset. Although these functions are being written for you, we highly recommend you go through the code and make yourself familiar as these are things you will be required to do very often when working on AI/ML projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-baptist",
   "metadata": {},
   "source": [
    "### Step - 5 (Flattening the images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-prophet",
   "metadata": {},
   "source": [
    "In the next cell we flatten each image into a single dimensional vector so that it is easy to process. You should be able to reason about the dimensions comfortable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-woman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Subsample the data for more efficient code execution in this exercise\n",
    "num_training = 5000\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "num_test = 500\n",
    "mask = list(range(num_test))\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# Reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-charleston",
   "metadata": {},
   "source": [
    "### Step - 6 (Apply all possible classification algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-clone",
   "metadata": {},
   "source": [
    "For each algo complete the following steps with appropriate plots, diagrams and code - \n",
    "1. Discuss the intuition behind each algorithm\n",
    "2. Mention pros and cons\n",
    "3. Discuss about Model parameters and Hyperparameters\n",
    "4. Discuss about overfitting and underfitting with relavant plots and code (Hint: Use cross validation and plot hyperparameter vs accuracy score)\n",
    "5. Discuss about train and test time/space complexities\n",
    "6. Measure the performance of model using various metrics and write in detail about each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining blank list for the input of scores of models\n",
    "model_accuracy=[]\n",
    "model_precision=[]\n",
    "model_recall=[]\n",
    "model_f1_score=[]\n",
    "model_roc_auc_score=[]\n",
    "model_train_time=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a function to train the model and evaluate the model\n",
    "def train_and_score (model,X_test,X_train,y_test,y_train, best_parameter=True, best_score=True):\n",
    "  '''\n",
    "  train the model and gives mse,rmse,r2,adj r2 score of the model\n",
    "  can be used for any model where y is not transformed \n",
    "  '''\n",
    "\n",
    "  #storing the start time of model training\n",
    "  start=time.time()\n",
    "  #training the model\n",
    "  model.fit(X_train,y_train)\n",
    "  #storing the time when the model ended trining \n",
    "  stop = time.time()\n",
    "  #calculating the total time taken for model training \n",
    "  time_min=round((stop - start)/60,4)\n",
    "  # printing the time taken to train the model\n",
    "  print(f\"Training time: {time_min}min\")\n",
    "  model_train_time.append(time_min)\n",
    "  \n",
    "  \n",
    "\n",
    "  # finding the best parameters of the model if any \n",
    "  if best_parameter==True:\n",
    "    # using try except method to avoid any error\n",
    "    try:\n",
    "      print(\"The best parameters found out to be :\" ,model.best_params_)\n",
    "    except:\n",
    "      pass\n",
    "  # finding best score of the model \n",
    "  if best_score==True:\n",
    "    try:\n",
    "      print(\" \\nwhere negative mean squared error is: \", model.best_score_,'\\n')\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "\n",
    "  #predicting the values of y from x via model\n",
    "  y_pred_test = model.predict(X_test)\n",
    "  y_pred_train = model.predict(X_train)\n",
    "\n",
    "  def score (actual,predicted,append=True):\n",
    "    '''\n",
    "    calcuating diffrent score matrix of the model\n",
    "    '''\n",
    "    # calculating f1_score score\n",
    "    f1 = f1_score(actual,predicted,average='micro')\n",
    "\n",
    "    \n",
    "    # printing all the scores\n",
    "    print(\"\\tThe f1 is \", f1)\n",
    "\n",
    "    # uploading all the score of the matrix in in the list \n",
    "    if append==True:\n",
    "      model_f1_score.append(f1)\n",
    "    else:\n",
    "      pass\n",
    "# printing the score by the help of coustom score function\n",
    "  # for train set\n",
    "  print('\\t\\t\\t\\tscore matrix for train')\n",
    "  print('*'*80)\n",
    "  score(y_train,y_pred_train,False)\n",
    "  print('\\n')\n",
    "  # for test set\n",
    "  print('\\t\\t\\t\\tscore matrix for test')\n",
    "  print('*'*80)\n",
    "  score(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49509c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the important librarys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, plot_precision_recall_curve\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b8aad",
   "metadata": {},
   "source": [
    "**Desision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0804812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# creating DecisionTree model\n",
    "DecisionTree = DecisionTreeClassifier(max_depth=10,max_leaf_nodes=45,criterion='entropy')\n",
    "\n",
    "# training and evaluating the DecisionTree\n",
    "train_and_score(DecisionTree,X_test, X_train, y_test, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff26e95",
   "metadata": {},
   "source": [
    "**Random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d85071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# creating Random Forest Regressor model\n",
    "rfc=RandomForestClassifier(n_jobs=-1,verbose=0)\n",
    "# training and evaluating the Random Forest Regressor\n",
    "train_and_score(rfc,X_test, X_train, y_test, y_train,best_parameter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b1293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c0abb0",
   "metadata": {},
   "source": [
    "**Implimenting grid search random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edcccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating RandomForestRegressor model\n",
    "rfc=RandomForestClassifier()\n",
    "rfc.get_params\n",
    "<bound method BaseEstimator.get_params of RandomForestClassifier()>\n",
    "# finding the best parameters for knn_model by gridsearchcv\n",
    "grid_values_rf = {'n_estimators': [100,125,150],'max_depth': [3,5,7],'criterion': ['entropy']}\n",
    "rfc_grid = GridSearchCV(estimator=rfc,param_grid = grid_values_rf, scoring='recall',cv=5,verbose=5,n_jobs=-1)\n",
    "# training and evaluating the Random Forest Regressor\n",
    "train_and_score(rfc_grid,X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc7799",
   "metadata": {},
   "source": [
    "**XGboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# creating XGBRegressor model\n",
    "xgbc=XGBClassifier()\n",
    "# training and evaluating the xgb_grid\n",
    "train_and_score(xgbc,X_test, X_train, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e72fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05767684",
   "metadata": {},
   "source": [
    "**implimenting grid search xgb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e254128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the best parameters for XGBRegressor by gridsearchcv\n",
    "xgbc_param={'n_estimators': [100,125,150],'max_depth': [7,10,15],'criterion': ['entropy']}\n",
    "xgbc_grid=GridSearchCV(estimator=xgbc,param_grid=xgbc_param,cv=3,scoring='recall',verbose=5,n_jobs=-1)\n",
    "# training and evaluating the xgb_grid\n",
    "train_and_score(xgbc_grid,X_test, X_train, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd8544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the best parameters for XGBRegressor by gridsearchcv\n",
    "xgbc_param={'n_estimators': [100,125,150],'max_depth': [7,10,15],'criterion': ['entropy']}\n",
    "xgbc_grid=GridSearchCV(estimator=xgbc,param_grid=xgbc_param,cv=3,scoring='recall',verbose=5,n_jobs=-1)\n",
    "# training and evaluating the xgb_grid\n",
    "train_and_score(xgbc_grid,X_test, X_train, y_test, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc2143",
   "metadata": {},
   "source": [
    "**CatBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "cb_model=CatBoostClassifier(verbose=0)\n",
    "# training and evaluating the PolynomialFeatures\n",
    "train_and_score(cb_model,X_test, X_train, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c166a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_model=CatBoostClassifier()\n",
    "# finding the best parameters for XGBRegressor by gridsearchcv\n",
    "cat_para={'n_estimators': [100,125,150],'max_depth': [3,5,7]}\n",
    "cat_grid=GridSearchCV(estimator=cb_model,param_grid=cat_para,cv=3,scoring='recall',verbose=3,n_jobs=-1)\n",
    "# training and evaluating the PolynomialFeatures\n",
    "train_and_score(cat_grid,X_test, X_train, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb9dcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b08758a",
   "metadata": {},
   "source": [
    "**lightgbm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "lgbc=lgb.LGBMClassifier()\n",
    "# training and evaluating the lgbr_grid\n",
    "train_and_score(lgbc, X_test, X_train, y_test, y_train)\n",
    "# finding the best parameters for XGBRegressor by gridsearchcv\n",
    "lgbc_para={'n_estimators': [100,125,150],'max_depth': [3,5,7]}\n",
    "lgbc_grid=GridSearchCV(estimator=lgbc,param_grid=lgbc_para,cv=3,scoring='recall',verbose=5,n_jobs=-1)\n",
    "# training and evaluating the lgbr_grid\n",
    "train_and_score(lgbc_grid,X_test, X_train, y_test, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-sleep",
   "metadata": {},
   "source": [
    "### Step - 7 (Compare all the models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-chinese",
   "metadata": {},
   "source": [
    "Conclude by following below mentioed steps -\n",
    "1. Plot a table showing all the algorithms used and corresponding performance metrics\n",
    "2. Is there any one algorithm that can be used on all the datasets? (Hint: Read about 'No Free Lunch' theorem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary to store all the metrices \n",
    "dict={'f1':model_f1_score,'train_time':model_train_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17125874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all models\n",
    "model_name=['Desision Tree','Random forest','Random forest Grid','XGboost','XGboost Grid','CATBoost','CATBoost Grid','lightGBM','lightGBM Grid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5199c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dictionary to dataframe\n",
    "matrix_df=pd.DataFrame.from_dict(dict,orient=\"index\",columns=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ac81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the transpose of the dataframe to make it more visual appealing\n",
    "matrix_df=matrix_df.transpose().reset_index().rename(columns={'index':'Models'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df.train_time.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
